{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset from Kaggle or Local, and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "dataset_path = \"\"\n",
    "using_kaggle_api = False\n",
    "\n",
    "if using_kaggle_api:\n",
    "    # Download latest version\n",
    "    dataset_path = kagglehub.dataset_download(\"a2015003713/militaryaircraftdetectiondataset\")\n",
    "else:\n",
    "    dataset_path = \"../data/archive_aircraft/crop\"\n",
    "    \n",
    "print(\"Path to dataset files:\", dataset_path)\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "# Get all files and labels\n",
    "folders = os.listdir(dataset_path)\n",
    "\n",
    "# Get all files and label from folder names\n",
    "for folder_name in folders:\n",
    "    # Get all files in folder_name: F16, A10, C130, etc.\n",
    "    files = os.listdir(os.path.join(dataset_path, folder_name))\n",
    "    print(\"Folder:\", folder_name, \"Number of files:\", len(files))\n",
    "    count = 0 \n",
    "    for f in files:\n",
    "        filepaths.append(os.path.join(dataset_path, folder_name, f))\n",
    "        labels.append(folder_name)\n",
    "        count += 1\n",
    "        print(f\"{folder_name}: #{count}\", end=\"\\r\")\n",
    "    print(\"#\" * 24)\n",
    "    \n",
    "\n",
    "dataset_dir = pd.DataFrame(data={\"filepaths\": filepaths, \"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dataset_dir, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "img_gen = ImageDataGenerator()\n",
    "target_size = (224, 224)\n",
    "\n",
    "train_gen = img_gen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    x_col=\"filepaths\", \n",
    "    y_col=\"labels\", \n",
    "    target_size=target_size, \n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"rgb\",\n",
    ")\n",
    "\n",
    "test_gen = img_gen.flow_from_dataframe(\n",
    "    test_df, \n",
    "    x_col=\"filepaths\", \n",
    "    y_col=\"labels\", \n",
    "    target_size=target_size, \n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"rgb\",\n",
    ")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"GPUs Available: \", gpus)\n",
    "\n",
    "if (gpus):\n",
    "    print(\"Setting memory growth for all GPUs\")\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Foudation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://keras.io/api/applications/\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/applications\n",
    "\"\"\"\n",
    "base_keras_model = tf.keras.applications.EfficientNetV2S(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling='max',\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    base_keras_model,\n",
    "    BatchNormalization(),\n",
    "    Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(.01)),\n",
    "    Dropout(.2),\n",
    "    # Dense(256, activation=\"relu\"),\n",
    "    # Dropout(.2),\n",
    "    # Output layer\n",
    "    Dense(74, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adamax(learning_rate=.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> **Training Model** <<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit(train_gen, validation_data=test_gen, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >== **Result Ploting** ==<\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = result.history[\"accuracy\"]\n",
    "train_loss = result.history[\"loss\"]\n",
    "\n",
    "validation_acc = result.history[\"val_accuracy\"]\n",
    "validation_loss = result.history[\"val_loss\"]\n",
    "\n",
    "index_acc_highest = np.argmax(validation_acc)\n",
    "acc_highest = validation_acc[index_acc_highest]\n",
    "\n",
    "index_lowest_loss = np.argmin(validation_loss)\n",
    "validation_lowest_loss = validation_loss[index_lowest_loss]\n",
    "\n",
    "loss_label = f\"best epoch = {str(index_lowest_loss + 1)} with loss = {str(validation_lowest_loss)}\"\n",
    "acc_label = f\"best epoch = {str(index_acc_highest + 1)} with acc = {str(acc_highest)}\"\n",
    "\n",
    "EPOCHS = [i+1 for i in range(len(train_acc))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.style.use(\"five_thirty_eight\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(EPOCHS, train_acc, \"blue\", label=\"Train Accuracy\")\n",
    "plt.plot(EPOCHS, validation_acc, \"pink\", label=\"Validation Accuracy\")\n",
    "plt.scatter(index_acc_highest + 1, acc_highest, color=\"red\", label=acc_label)\n",
    "plt.title(\"Train & Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(EPOCHS, train_loss, \"blue\", label=\"Train Loss\")\n",
    "plt.plot(EPOCHS, validation_loss, \"pink\", label=\"Validation Loss\")\n",
    "plt.scatter(index_lowest_loss + 1, validation_lowest_loss, color=\"red\", label=loss_label)\n",
    "plt.title(\"Train & Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "model.save(f\"aircraft_model_{train_acc:.02f}.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
